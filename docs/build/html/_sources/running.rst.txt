.. _Running:
  
=====================
Running
=====================

We highly recommend creating a screen before running MicroExonator

.. code-block:: bash

   screen -S session_name  #choose a meaning full name to you

To activate snakemake enviroment

.. code-block:: bash

   conda activate snakemake_env


In case you already have an older version of conda use instead:

.. code-block:: bash

   source activate snakemake_env

Then run

.. code-block:: bash

   snakemake -s MicroExonator.skm  --cluster-config cluster.json --cluster {cluster system params} --use-conda -k  -j {number of parallel jobs}

Before running it is recommended to check if SnakeMake can corretly generate all the steps given your input. To do this you can carry out a dry-run using the -np parameter:

.. code-block:: bash

   snakemake -s MicroExonator.skm  --cluster-config cluster.json --cluster {cluster system params} --use-conda -k  -j {number of parallel jobs} -np

The dry-run will display all the steps and commands that will be excecuted. If the dry-run cannot be initiated, make sure that you are running MicroExonator from inside the folder you cloned from this repository. Also make sure you have the right configuration inside config.yaml.

Notice that you should use --cluster only if you are working in a computer cluster that uses a queuing systems. We provide an example of cluster.json to work with lsf and in that case the cluster system params should be "bsub -n {cluster.nCPUs} -R {cluster.resources} -c {cluster.tCPU} -G {cluster.Group} -q {cluster.queue} -o {cluster.output} -e {cluster.error} -M {cluster.memory}". The number of parallel jobs can be a positive integer, the appropriate value depends on the capacity of your machine but for most users a value between 5 and 50 is appropriate.

If you want to process a large dataset, we recommend to set Optimize_hard_drive as T. If you tho this, you can also run the pipeline in two steps:

.. code-block:: bash

   snakemake -s MicroExonator.skm  --cluster-config cluster.json --cluster {cluster system params} --use-conda -k  -j {number of parallel jobs} discovery

If you do not have space to have a let MicroExonator to have a complete copy of the data you want to process, you can limmit the number of jobs. For instance if you limit the number of jobs to 20, you can make sure no more than 20 fastq files will be stored at `FASTQ/` directory (if `Optimize_hard_drive` is set as `T`) 

If you are working remotelly, the connection is likely to die before MicroExonator finish. However, as long as you are working within an screen, loggin off will not kill snakemake. To list your active screens you can do:

.. code-block:: bash

   screen -ls


To reattached and detach screens just use:

.. code-block:: bash

   screen -r session_name  # only detached screen can be reattached  

   screen -d session_name


** Alternative splicing analyses **

Microexonator can couple the annotation and quantification of microexon with other downstream tools to asses alternative splicing changing. We have incorporated [whippet](https://github.com/timbitz/Whippet.jl) into a downstream snakemake workflow that can directly use the microexon annotation and quantification files to assess differential inclusion of microexons between multiple conditions. For this porpuse we recomend to install a custumised conda enviroment that has snkamemake and julia 0.6.1 installed. This can be don by using the following command from inside a MicroExonator folder:

.. code-block:: bash

   `conda env create -f Whippet/julia_0.6.1.yaml`

Where `Whippet/julia_0.6.1.yam` is yaml file with the recipe to create a stable environment with julia 0.6.1 and snakemake.

Then, activate the newly created enviroment:

.. code-block:: bash
 
   `source activate julia_0.6.1`


Enter julia's interactive mode:

.. code-block:: bash

   `julia`


Install Whippet:

   `Pkg.add("Whippet")`


Exit interactive julia session (`control + d`) and find Whippet's binary folder, that should be inside of your miniconda environment folder. Once you find the path to this folder, add it to `config.yaml` writing the following lines:

.. code-block:: bash

    whippet_bin_folder : path/to/miniconda/envs/julia_0.6.1/share/julia/site/v0.6/Whippet/bin
    Gene_anontation_GTF : path/to/gene_annotation.gtf
    whippet_delta:
        Control_vs_TreatmentA :
            A : SRR309144,SRR309143,SRR309142,SRR309141
            B : SRR309136,SRR309135,SRR309134,SRR309133
        Control_vs_TreatmentB :
            A : SRR309140,SRR309139
            B : SRR309138,SRR309137


Where `whippet_delta` correspond to a dictionary that has all the comparions betweeen samples that you would like to do. Each comparison can have an arbitrary name (here as `Control_vs_TreatmentA` and `Control_vs_TreatmentB`). For each comparison two groups of samples are compared; `A` and `B`. These correspond to comma-separated list of the replicates for the conditions you are interested to compare.


Then we recomend to do a `dry-run` to check that all the inputs are in place:

.. code-block:: bash
    
   snakemake -s MicroExonator.skm  --cluster-config cluster.json --cluster {cluster system params} --use-conda -k  -j {number of parallel jobs} differential_inclusion -np



